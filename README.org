** Usage

#+begin_src sh
# download workflow
git clone https://github.com/thackl/detectEVE
cd detectEVE

# install dependencies via conda or mamba (https://github.com/conda-forge/miniforge)
mamba create -n detectEVE
mamba activate detectEVE
mamba env update --file workflow/envs/env.yaml

# run detectEVE
./detectEVE -h                  # show help
./detectEVE --setup-databases   # download and prep rvdb and uniref50

./detectEVE *.fasta             # analyze local genomes
./detectEVE -a ACC [ACC ...]    # download and analyze NCBI WGS genomes
#+end_src

See [[https://www.ncbi.nlm.nih.gov/Traces/wgs/][NCBI SRA WGS]] for downloadable accessions. Downloaded genomes can be found at
=genomes/<accession>.fna=.

See [[Advanced database setup]] for alternatives and customization of databases.

See [[Known issues]] and https://github.com/thackl/detectEVE/issues for questions,
problems or feedback.

** Output
The pipeline produces the following final files in =results/=:
- =<genome_id>-validatEVEs.tsv= - best hit of the EVE with evidence and confidence
  annotation (exogenous virus: high, eve-ish description & hypothetical protein:
  low)
- =<genome_id>-validatEVEs.fna= - validatEVEs nucleotide sequences
- =<genome_id>-validatEVEs.pdf= - graphical overview of hit distribution for validatEVEs

[[file:workflow/detectEVE-output-example.png]]

** Workflow overview and background
detectEVE is based on the EVE search strategy developed by S. Lequime and
previously used in publications ...

The current workflow involves the following steps:

[[file:workflow/detectEVE-workflow.png]]

** Advanced database setup

If you need *databases in a different location* you can adjust =db_dir= in
=config.yaml= to whatever suits your system.

If you prefer to *handle downloads manually or use existing files*, copy any
file you don't want detectEVE to download automatically into =databases/= (or
the respective =config.yaml/db_dir=) before running =--setup-databases=.

Note though, unless you add =--notemp= to the =--snake=-arguments, all but the
final diamond-formatted database files will be deleted from =databases/= at the
end of the setup phase.

#+begin_src sh
cd databases/

# Latest RVDB
url=https://rvdb-prot.pasteur.fr/ && 
db=$(curl -fs $url | grep -oPm1 'files/U-RVDBv[0-9.]+-prot.fasta.xz')
curl $url/$db -o rvdb100.faa.xz

# UniRef50
wget https://ftp.uniprot.org/pub/databases/uniprot/uniref/uniref50/uniref50.fasta.gz

# NCBI taxonomy stuff
wget https://ftp.ncbi.nlm.nih.gov/pub/taxonomy/accession2taxid/prot.accession2taxid.FULL.gz
wget https://ftp.ncbi.nih.gov/pub/taxonomy/taxdump.tar.gz
tar -xzf taxdump.tar.gz nodes.dmp names.dmp
#+end_src


** Known issues
*tidyverse stringi libicui*
If you encounter an error related to /tidyverse/stringi/libicui18n.so.58/, try
reinstalling =stringi= locally. To restart the workflow from where it failed,
just run the same command again.

#+begin_src sh
mamba remove r-stringi r-tidyverse
R -e 'install.packages("stringi")'
mamba install r-tidyverse
#+end_src
