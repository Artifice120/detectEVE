# relative to workflow dir
wf_dir=config["wf_dir"]
configfile: f"{wf_dir}/config.yaml"

import pandas as pd
import glob
import requests
import re
import os
from os.path import exists

scripts=os.path.join(wf_dir, "workflow/scripts")

dbs=config["db_dir"]
if not os.path.isabs(dbs):
    dbs=os.path.join(wf_dir, dbs)


# read accessions for genome download
def acc():
    tsv_acc = []
    if(os.path.exists("accessions.csv")):
        df = pd.read_csv("accessions.csv", sep=",")
        tsv_acc = df.iloc[:, 0].tolist()

    fna_acc = glob.glob("genomes/*.fna")
    fna_acc = [x.removeprefix("genomes/").removesuffix(".fna") for x in fna_acc]

    return list(set(tsv_acc + fna_acc))
    

## Pseudo-rules
rule all:
    input:
        # check_dbs(),
        expand("results/{asm}-validatEVEs.{ext}", asm=acc(), ext=["fna", "pdf", "tsv"])

rule setup:
    input:
        f"{dbs}/uniref50.dmnd",
        f"{dbs}/rvdb80.dmnd"
        #f"{dbs}/rvdb80-uniref50.dmnd"
    
## genome download ----------------------------------------------------------##
rule download_genome:
    output:
        fna=temp("genomes/{asm}.fna"),
        htm=temp("genomes/{asm}.htm")
    log: "genomes/{asm}.log"
    # a bit dirty to prevent mulitple downloads to run at same time
    threads: workflow.cores * 0.60
    # I tried ftp before but not all accessions seem to be there (e.g. Cafeteria
    # VLTM01, ...)  Multiple links are split genomes, and should be concatenated
    # (e.g. NIUR01.1.fsa_nt.gz, NIUR01.2.fsa_nt.gz, ...)
    shell:
        "{scripts}/download-traces.sh {wildcards.asm} {output.htm} > {output.fna} 2> {log}"

## analysis -----------------------------------------------------------------##
rule search_assemblies:
    input: "genomes/{asm}.fna"
    output:
        tmp="results/{asm}-search.o6",
        bed="results/{asm}-search.bed"
    threads: workflow.cores
    shell:
        "{scripts}/diamond-chopped.sh blastx"
        " --query {input} --out {output.tmp} --threads {threads}"
        " --db {dbs}/{config[search][db]} {config[search][taxonlist]}"
        " --evalue {config[search][evalue]} {config[search][other_args]}"
        " -W {config[search][chop_window]} -S {config[search][chop_step]}\n"
        "{scripts}/blast2bed -qa {output.tmp} |"
        " {scripts}/bed-top -m {config[search][min_length_aa]} > {output.bed}"

rule extract_putatEVEs:
    input:
        bed="results/{asm}-search.bed",
        fna="genomes/{asm}.fna"
    output: "results/{asm}-putatEVEs-unmasked.fna"
    shell:
        "seqkit subseq --bed {input} > {output}"

rule masksearch_putatEVEs:
    input: "results/{asm}-putatEVEs-unmasked.fna" 
    output:
        o6="results/{asm}-mask.o6",
        bed="results/{asm}-mask.bed"
    threads: workflow.cores
    shell:
        "diamond blastx --query {input} --threads {threads}"
        " --db {dbs}/{config[mask][db]}"
        " --evalue {config[search][evalue]}"
        " {config[search][other_args]}"
        " --out {output.o6}\n"
        "{scripts}/blast2bed -qa {output.o6} |"
        " {scripts}/bed-top -m {config[search][min_length_aa]} > {output.bed}"

rule mask_putatEVEs:
    input:
        fna="results/{asm}-putatEVEs-unmasked.fna",
        bed="results/{asm}-mask.bed"
    output: "results/{asm}-putatEVEs.fna"
    shell:
        "bedtools maskfasta -fi {input.fna} -bed {input.bed} -fo {output} -fullHeader"
       
        
rule retrosearch_putatEVEs_udb:
    input: "results/{asm}-putatEVEs.fna" 
    output: "results/{asm}-retro-udb.o6",
    threads: workflow.cores
    shell:
        "diamond blastx --query {input} --threads {threads}"
        " --db {dbs}/{config[retrosearch][db]}"
        " --evalue {config[retrosearch][evalue]}"
        " {config[retrosearch][other_args]}"
        " --out {output}\n"

rule retrosearch_putatEVEs_vdb:
    input: "results/{asm}-putatEVEs.fna" 
    output: "results/{asm}-retro-vdb.o6",
    threads: workflow.cores
    shell:
        "diamond blastx --query {input} --threads {threads}"
        " --db {dbs}/{config[search][db]}"
        " --evalue {config[search][evalue]}"
        " {config[search][other_args]}"
        " --out {output}\n"

rule retrosearch_putatEVEs_merge:
    input:
        udb="results/{asm}-retro-udb.o6",
        vdb="results/{asm}-retro-vdb.o6",
    output:
        bed=temp("results/{asm}-retro-notax.bed")
    threads: workflow.cores
    shell:
        "(sed 's/$/\tVDB/' {input.vdb}; sed 's/$/\tUDB/' {input.udb}) |"
        " {scripts}/blast2bed -qa | {scripts}/bed-top -tk 20 > {output.bed}"

rule taxize_putatEVEs:
    input: "results/{asm}-retro-notax.bed"
    output: "results/{asm}-retro.bed"
    threads: workflow.cores
    shell:
        # parse taxid to column, get lineage
        """
        perl -pe '($t) = /taxid[ :=](\d+)/i; $t//=""; s/$/\t$t/;' {input} |
        taxonkit reformat -P -I 16 -a -F -f "{{k}};{{K}};{{p}};{{c}};{{o}};{{f}};{{g}};{{s}}" > {output};
        """
        
rule validate_putatEVEs:
    input: "results/{asm}-retro.bed"
    output:
        tsv="results/{asm}-validatEVEs.tsv",
        pdf="results/{asm}-validatEVEs.pdf"
    params: "{asm}"
    shell:
        "Rscript --vanilla {scripts}/validate.R -p {params} {config[validate][args]} {input} {output.tsv} {output.pdf}"

rule extract_validatEVEs:
    input:
        fna="results/{asm}-putatEVEs.fna",
        tsv="results/{asm}-validatEVEs.tsv"
    output: "results/{asm}-validatEVEs.fna" 
    params: "{asm}"
    shell:
        "cut -f6 {input.tsv} | sed '1d' | "
        " seqkit faidx {input.fna} --infile-list - -f |"
        " perl -pe 's/^>/sprintf(\">%s_EVE%03d \", {params}, ++$i)/e' > {output}"





## databases downloads ------------------------------------------------------------##
rule download_ncbi_taxonomy:
    # a bit dirty to prevent mulitple downloads to run at same time
    threads: workflow.cores * 0.60
    output:
        targz=temp("{dbs}/taxdump.tar.gz"),
        nodes=temp("{dbs}/nodes.dmp"),
        names=temp("{dbs}/names.dmp")
    shell:
        "curl https://ftp.ncbi.nih.gov/pub/taxonomy/taxdump.tar.gz -o {output.targz}\n"
        "cd {dbs}/ && tar -xzf taxdump.tar.gz nodes.dmp names.dmp"

rule download_ncbi_acc2tax:
    # a bit dirty to prevent mulitple downloads to run at same time
    threads: workflow.cores * 0.60
    output: temp("{dbs}/prot.accession2taxid.FULL.gz")
    shell: "curl https://ftp.ncbi.nlm.nih.gov/pub/taxonomy/accession2taxid/prot.accession2taxid.FULL.gz -o {output}"

      
rule download_uniref:
    # a bit dirty to prevent mulitple downloads to run at same time
    threads: workflow.cores * 0.60
    output: temp("{dbs}/uniref50.fasta.gz")
    shell:
        "curl https://ftp.uniprot.org/pub/databases/uniprot/uniref/uniref50/uniref50.fasta.gz -o {output}"
         

rule download_rvdb:
    # a bit dirty to prevent mulitple downloads to run at same time
    threads: workflow.cores * 0.60
    output: temp("{dbs}/rvdb100.faa.xz"),  # do not mark temp(), could be userspecified location
    shell:
        """
        set +o pipefail;
        url=https://rvdb-prot.pasteur.fr/
        db=$(curl -fs $url | grep -oPm1 'files/U-RVDBv[0-9.]+-prot.fasta.xz')
        curl $url/$db -o {output}
        """

## DEPRECATED - work with NR
# rule download_nr:
#     # a bit dirty to prevent mulitple downloads to run at same time
#     threads: workflow.cores * 0.60
#     output: temp("{dbs}/nr.faa.gz")
#     shell: "curl https://ftp.ncbi.nlm.nih.gov/blast/db/FASTA/nr.gz -o nr.faa.gz"

## databases prep ------------------------------------------------------------##
rule setup_uniref:
    input:
        faagz="{dbs}/uniref50.fasta.gz", # do not mark temp(), could be userspecified location
        nodes="{dbs}/nodes.dmp",
        names="{dbs}/names.dmp"
    output:
        taxonmap=temp("{dbs}/uniref50-acc2tax.tsv.gz"),
        dmnd="{dbs}/uniref50.dmnd"
    shell:
        """
        ( echo -e "accession.version\ttaxid";
          gzip -cd {input.faagz} |
          perl -ne 'if(/>UniRef50_(\S+).*TaxID=(\d+)/){{print $1,"\t",$2,"\n"}}'
        ) | gzip -c > {output.taxonmap}
        """
        'diamond makedb --in {input.faagz} --db {output.dmnd}'
        ' --taxonmap {output.taxonmap} --taxonnodes {input.nodes} --taxonnames {input.names}'

rule unpack_rvdb:
    # a bit dirty to prevent mulitple downloads to run at same time
    input:
        xz="{dbs}/rvdb100.faa.xz",
    output:
        faa=temp("{dbs}/rvdb100.faa")
    shell:
        """
        # unpack
        xz -cd {input.xz} | seqkit seq -m 60 - | seqkit rmdup -s - | perl -pe 's/^>(acc\|\w+\|)([^|]+)/>$2 $1$2/' > {output.faa}
        """
        
# 16GB RAM should suffice: "Approximated maximum memory consumption: 10179M"
rule cluster_rvdb:
    input:
        "{dbs}/rvdb100.faa"
    output:
        faa=temp("{dbs}/rvdb80.faa"),
        cls=temp("{dbs}/rvdb80.faa.clstr"),
        faagz=temp("{dbs}/rvdb80.faa.gz")
    threads: workflow.cores
    shell:
        """
        cd-hit -c .8 -i {input} -o {output.faa} -M 16000 -T {threads}
	gzip -k {output.faa}
        """

rule prep_rvdb_taxonomy:
    # a bit dirty to prevent mulitple downloads to run at same time
    threads: workflow.cores * 0.60
    input:
        faa="{dbs}/rvdb80.faa",
        mapgz="{dbs}/prot.accession2taxid.FULL.gz"
    output:
        acc=temp("{dbs}/rvdb80-acc.tsv"),
        tax=temp("{dbs}/rvdb80-acc2tax.tsv"),
        taxgz=temp("{dbs}/rvdb80-acc2tax.tsv.gz")
    threads: 10
    shell:
        """
        grep '^>' {input.faa} | cut -f3 -d'|' > {output.acc} 
        csvtk -tT grep -j {threads} -P {output.acc} {input.mapgz} > {output.tax}
        gzip -k {output.tax}
        """

rule setup_rvdb:
    input:
        faagz="{dbs}/rvdb80.faa.gz",
        mapgz="{dbs}/rvdb80-acc2tax.tsv.gz",
        nodes="{dbs}/nodes.dmp",
        names="{dbs}/names.dmp"
    output:
        dmnd="{dbs}/rvdb80.dmnd"
    shell:
        'diamond makedb --in {input.faagz} --db {output.dmnd} --taxonmap {input.mapgz} --taxonnodes {input.nodes} --taxonnames {input.names}'

rule setup_retrodb:
    input:
        rvdb_faagz="{dbs}/rvdb80.faa.gz",
        rvdb_taxgz="{dbs}/rvdb80-acc2tax.tsv.gz",
        uniref_faagz="{dbs}/uniref50.fasta.gz",
        uniref_taxgz="{dbs}/uniref50-acc2tax.tsv.gz",
        nodes="{dbs}/nodes.dmp",
        names="{dbs}/names.dmp"
    output:
        taxgz=temp("{dbs}/rvdb80-uniref50-acc2tax.tsv.gz"),
        dmnd="{dbs}/rvdb80-uniref50.dmnd"
    shell:
        """
        cp {input.rvdb_taxgz} {output.taxgz}
        zcat {input.uniref_taxgz} | sed -n '2,$p' | gzip -c >> {output.taxgz}
        zcat {input.rvdb_faagz} {input.uniref_faagz} |
        diamond makedb --db {output.dmnd} --taxonmap {output.taxgz} --taxonnodes {input.nodes} --taxonnames {input.names}
        """

## DEPRECATED
# rule setup_nr:
#     input:
#         faagz="{dbs}/nr.faa.gz",
#         mapgz="{dbs}/prot.accession2taxid.FULL.gz",
#         nodes="{dbs}/nodes.dmp",
#         names="{dbs}/names.dmp",
#     output:
#         dmnd="{dbs}/nr.dmnd"
#     shell:
#         'diamond makedb --in {input.faagz} --db {output.dmnd} --taxonmap {input.mapgz} --taxonnodes {input.nodes} --taxonnames {input.names}'
