# relative to workflow dir
wf_dir=config["wf_dir"]
configfile: f"{wf_dir}/config.yaml"

import pandas as pd
import glob
import requests
import re
import os
from os.path import exists

scripts=os.path.join(wf_dir, "workflow/scripts")

dbs=config["db_dir"]
if not os.path.isabs(dbs):
    dbs=os.path.join(wf_dir, dbs)


# read accessions for genome download
def acc():
    df = pd.read_csv("genomes.tsv", sep="\t")
    tsv_acc = df['#accession'].tolist()

    fna_acc = glob.glob("genomes/*.fna")
    fna_acc = [x.removeprefix("genomes/").removesuffix(".fna") for x in fna_acc]

    return list(set(tsv_acc + fna_acc))

## Pseudo-rules
rule all:
    input:
        # check_dbs(),
        expand("results/{asm}-validatEVEs.{ext}", asm=acc(), ext=["fna", "pdf", "tsv"])

rule setup:
    input:
        f"{dbs}/uniref50.dmnd",
        f"{dbs}/rvdb80.dmnd",
        f"{dbs}/rvdb80-uniref50.dmnd"
    
## genome download ----------------------------------------------------------##
rule download_genome:
    output: "genomes/{asm}.fna"
    # a bit dirty to prevent mulitple downloads to run at same time
    threads: workflow.cores * 0.60
    shell:
        "(cd genomes && {scripts}/download-ncbi-genomes.pl {wildcards.asm})"

## analysis -----------------------------------------------------------------##
rule search_assemblies:
    input: "genomes/{asm}.fna"
    output: "results/{asm}-search.o6"
    threads: workflow.cores
    shell:
        "{scripts}/diamond-chopped.sh blastx "
        "--query {input} --out {output} --threads {threads} "
        "--db {dbs}/{config[search][db]} --evalue {config[search][evalue]} "
        "-W {config[search][chop_window]} -S {config[search][chop_step]} "
        "{config[search][other_args]}\n"

rule clean_search_hits:
    input: "results/{asm}-search.o6"
    output: "results/{asm}-search-cleaned.o6"
    shell:
        "uniq {input} | perl -ane 'print if $F[3] > {config[search][min_length_aa]}' > {output}"
        
rule extract_putatEVEs:
    input:
        fna="genomes/{asm}.fna",
        o6="results/{asm}-search-cleaned.o6"
    output: "results/{asm}-putatEVEs.fna"
    shell:
        "{scripts}/blast2bed -q {input.o6} | seqkit subseq --bed /dev/fd/0 {input.fna} > {output}"

rule retrosearch_putatEVEs:
    input: "results/{asm}-putatEVEs.fna" 
    output: "results/{asm}-retro.o6"
    threads: workflow.cores
    shell:
        "diamond blastx --query {input} --out {output} --threads {threads} "
        "--db {dbs}/{config[retrosearch][db]} --evalue {config[retrosearch][evalue]} "
        "{config[retrosearch][other_args]} "

rule validate_putatEVEs:
    input: "results/{asm}-retro.o6"
    output:
        tsv="results/{asm}-validatEVEs.tsv",
        pdf="results/{asm}-validatEVEs.pdf"
    shell:
        "{scripts}/validate.R {input} {output.tsv} {output.pdf}"

rule extract_validatEVEs:
    input:
        fna="results/{asm}-putatEVEs.fna",
        tsv="results/{asm}-validatEVEs.tsv"
    output: "results/{asm}-validatEVEs.fna" 
    shell:
        "cut -f1 {input.tsv} | seqkit grep -f - {input.fna} > {output}"






## databases -----------------------------------------------------------------##
rule download_ncbi_taxonomy:
    # a bit dirty to prevent mulitple downloads to run at same time
    threads: workflow.cores * 0.60
    output:
        targz=temp("{dbs}/taxdump.tar.gz"),
        nodes="{dbs}/nodes.dmp",
        names="{dbs}/names.dmp"
    shell:
        "curl https://ftp.ncbi.nih.gov/pub/taxonomy/taxdump.tar.gz -o {output.targz}\n"
        "cd {dbs}/ && tar -xzf taxdump.tar.gz nodes.dmp names.dmp"

rule download_ncbi_acc2tax:
    # a bit dirty to prevent mulitple downloads to run at same time
    threads: workflow.cores * 0.60
    output: "{dbs}/ncbi-acc2tax.gz"
    shell: "curl https://ftp.ncbi.nlm.nih.gov/pub/taxonomy/accession2taxid/prot.accession2taxid.FULL.gz -o {output}"

rule download_nr:
    # a bit dirty to prevent mulitple downloads to run at same time
    threads: workflow.cores * 0.60
    output: "{dbs}/nr.faa.gz"
    shell: "curl https://ftp.ncbi.nlm.nih.gov/blast/db/FASTA/nr.gz -o nr.faa.gz"

rule setup_nr:
    input:
        faagz="{dbs}/nr.faa.gz",
        mapgz="{dbs}/ncbi-acc2tax.gz",
        nodes="{dbs}/nodes.dmp",
        names="{dbs}/names.dmp",
    output:
        dmnd="{dbs}/nr.dmnd"
    shell:
        'diamond makedb --in {input.faagz} --db {output.dmnd} --taxonmap {input.mapgz} --taxonnodes {input.nodes} --taxonnames {input.names}'
           
rule download_uniref:
    # a bit dirty to prevent mulitple downloads to run at same time
    threads: workflow.cores * 0.60
    output: "{dbs}/uniref50.fasta.gz"
    shell:
        "curl https://ftp.uniprot.org/pub/databases/uniprot/uniref/uniref50/uniref50.fasta.gz -o {output}"
         
rule setup_uniref:
    input:
        faagz="{dbs}/uniref50.fasta.gz",
        nodes="{dbs}/nodes.dmp",
        names="{dbs}/names.dmp"
    output:
        taxonmap=temp("{dbs}/uniref50-acc2tax.tsv.gz"),
        dmnd="{dbs}/uniref50.dmnd"
    shell:
        """
        ( echo -e "accession.version\ttaxid";
          gzip -cd {input.faagz} |
          perl -ne 'if(/>UniRef50_(\S+).*TaxID=(\d+)/){{print $1,"\t",$2,"\n"}}'
        ) | gzip -c > {output.taxonmap}
        """
        'diamond makedb --in {input.faagz} --db {output.dmnd}'
        ' --taxonmap {output.taxonmap} --taxonnodes {input.nodes} --taxonnames {input.names}'

rule download_rvdb:
    # a bit dirty to prevent mulitple downloads to run at same time
    threads: workflow.cores * 0.60
    output:
        xz=temp("{dbs}/rvdb100.faa.xz"),
        faa=temp("{dbs}/rvdb100.faa")
    shell:
        """
        set +o pipefail;
        url=https://rvdb-prot.pasteur.fr/
        db=$(curl -fs $url | grep -oPm1 'files/U-RVDBv[0-9.]+-prot.fasta.xz')
        curl $url/$db -o {output.xz}
        # unpack
        xz -cd {output.xz} | seqkit seq -m 60 - | seqkit rmdup -s - | perl -pe 's/^>(acc\|\w+\|)([^|]+)/>$2 $1$2/' > {output.faa}
        """

# 16GB RAM should suffice: "Approximated maximum memory consumption: 10179M"
rule cluster_rvdb:
    input:
        "{dbs}/rvdb100.faa"
    output:
        faa=temp("{dbs}/rvdb80.faa"),
        cls=temp("{dbs}/rvdb80.faa.clstr"),
	faagz="{dbs}/rvdb80.faa.gz"
    threads: workflow.cores
    shell:
        """
        cd-hit -c .8 -i {input} -o {output.faa} -M 16000 -T {threads}
	gzip -k {output.faa}
        """

rule download_rvdb_taxonomy:
    # a bit dirty to prevent mulitple downloads to run at same time
    threads: workflow.cores * 0.60
    input:
        "{dbs}/rvdb80.faa"
    output:
        acc=temp("{dbs}/rvdb80-acc.tsv"),
        tax=temp("{dbs}/rvdb80-acc2tax.tsv"),
        taxgz=temp("{dbs}/rvdb80-acc2tax.tsv.gz")
    threads: 10
    shell:
        """
        grep '^>' {input} | cut -f3 -d'|' > {output.acc} 
        curl https://ftp.ncbi.nlm.nih.gov/pub/taxonomy/accession2taxid/prot.accession2taxid.FULL.gz |
        csvtk -tT grep -j {threads} -P {output.acc} > {output.tax}
        gzip -k {output.tax}
        """

# DEPRECATED - can get taxinfo from taxified diamond DB
# rule taxify_rvdb:
#     input:
#         faa="{dbs}/rvdb80r.faa",
#         tax="{dbs}/rvdb80-acc2tax.tsv"
#     output:
#         faa=temp("{dbs}/rvdb80.faa"),
#         gz="{dbs}/rvdb80.faa.gz"
#     threads: 10
#     shell:
#         """
#         seqkit replace -j {threads} -p 'acc\\|[^\\|]+\\|([^\\|]+).*' -r '${{1}} ${{0}} taxid={{kv}}' -k {input.tax} {input.faa} -m NA > {output.faa}
#         gzip -k {output.faa}
#         """

rule setup_rvdb:
    input:
        faagz="{dbs}/rvdb80.faa.gz",
        mapgz="{dbs}/rvdb80-acc2tax.tsv.gz",
        nodes="{dbs}/nodes.dmp",
        names="{dbs}/names.dmp"
    output:
        dmnd="{dbs}/rvdb80.dmnd"
    shell:
        'diamond makedb --in {input.faagz} --db {output.dmnd} --taxonmap {input.mapgz} --taxonnodes {input.nodes} --taxonnames {input.names}'

rule setup_retrodb:
    input:
        rvdb_faagz="{dbs}/rvdb80.faa.gz",
        rvdb_taxgz="{dbs}/rvdb80-acc2tax.tsv.gz",
        uniref_faagz="{dbs}/uniref50.fasta.gz",
        uniref_taxgz="{dbs}/uniref50-acc2tax.tsv.gz",
        nodes="{dbs}/nodes.dmp",
        names="{dbs}/names.dmp"
    output:
        taxgz=temp("{dbs}/rvdb80-uniref50-acc2tax.tsv.gz"),
        dmnd="{dbs}/rvdb80-uniref50.dmnd"
    shell:
        """
        cp {input.rvdb_taxgz} {output.taxgz}
        zcat {input.uniref_taxgz} | sed -n '2,$p' | gzip -c >> {output.taxgz}
        zcat {input.rvdb_faagz} {input.uniref_faagz} |
        diamond makedb --db {output.dmnd} --taxonmap {output.taxgz} --taxonnodes {input.nodes} --taxonnames {input.names}
        """
