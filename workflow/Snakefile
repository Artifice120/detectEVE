# relative to workflow dir
wf_dir=config["wf_dir"]
configfile: f"{wf_dir}/config.yaml"

import pandas as pd
import glob
import requests
import re
import os
from os.path import exists

scripts=os.path.join(wf_dir, "workflow/scripts")

dbs=config["db_dir"]
if not os.path.isabs(dbs):
    dbs=os.path.join(wf_dir, dbs)


# read accessions for genome download
def acc():
    tsv_acc = []
    if(os.path.exists("accessions.csv")):
        df = pd.read_csv("accessions.csv", sep=",")
        tsv_acc = df.iloc[:, 0].tolist()

    fna_acc = glob.glob("genomes/*.fna")
    fna_acc = [x.removeprefix("genomes/").removesuffix(".fna") for x in fna_acc]

    return list(set(tsv_acc + fna_acc))
    

## Pseudo-rules
rule all:
    input:
        # check_dbs(),
        expand("results/{asm}-validatEVEs.{ext}", asm=acc(), ext=["fna", "pdf", "tsv"])

rule setup:
    input:
        f"{dbs}/uniref50.dmnd",
        f"{dbs}/rvdb80.dmnd",
        f"{dbs}/rvdb80-uniref50.dmnd"
    
## genome download ----------------------------------------------------------##
rule download_genome:
    output: temp("genomes/{asm}.fna")
    # a bit dirty to prevent mulitple downloads to run at same time
    threads: workflow.cores * 0.60
    # I tried ftp before but not all accessions seem to be there (e.g. Cafeteria VLTM01, ...)
    shell:
        """(
        set +o pipefail;
        url=$(
            curl "https://www.ncbi.nlm.nih.gov/Traces/wgs/{wildcards.asm}?display=download" |
            grep -oP 'https://sra-download.ncbi.nlm.nih.gov/[^"]*.fsa_nt.gz' |
            tail -n1) # the newest version
        curl $url | gzip -cd > {output})
        """

## analysis -----------------------------------------------------------------##
rule search_assemblies:
    input: "genomes/{asm}.fna"
    output: "results/{asm}-search.o6"
    threads: workflow.cores
    shell:
        "{scripts}/diamond-chopped.sh blastx "
        "--query {input} --out {output} --threads {threads} "
        "--db {dbs}/{config[search][db]} --evalue {config[search][evalue]} "
        "-W {config[search][chop_window]} -S {config[search][chop_step]} "
        "{config[search][other_args]}\n"

rule clean_search_hits:
    input: "results/{asm}-search.o6"
    output: "results/{asm}-search-cleaned.o6"
    shell:
        "uniq {input} | perl -ane 'print if $F[3] > {config[search][min_length_aa]}' > {output}"
        
rule extract_putatEVEs:
    input:
        fna="genomes/{asm}.fna",
        o6="results/{asm}-search-cleaned.o6"
    output: "results/{asm}-putatEVEs.fna"
    shell:
        "{scripts}/blast2bed -q {input.o6} | seqkit subseq --bed /dev/fd/0 {input.fna} > {output}"

rule retrosearch_putatEVEs:
    input: "results/{asm}-putatEVEs.fna" 
    output: "results/{asm}-retro.o6"
    threads: workflow.cores
    shell:
        "diamond blastx --query {input} --out {output} --threads {threads} "
        "--db {dbs}/{config[retrosearch][db]} --evalue {config[retrosearch][evalue]} "
        "{config[retrosearch][other_args]} "

rule validate_putatEVEs:
    input: "results/{asm}-retro.o6"
    output:
        tsv="results/{asm}-validatEVEs.tsv",
        pdf="results/{asm}-validatEVEs.pdf"
    shell:
        "{scripts}/validate.R {input} {output.tsv} {output.pdf}"

rule extract_validatEVEs:
    input:
        fna="results/{asm}-putatEVEs.fna",
        tsv="results/{asm}-validatEVEs.tsv"
    output: "results/{asm}-validatEVEs.fna" 
    shell:
        "cut -f1 {input.tsv} | seqkit grep -f - {input.fna} > {output}"






## databases downloads ------------------------------------------------------------##
rule download_ncbi_taxonomy:
    # a bit dirty to prevent mulitple downloads to run at same time
    threads: workflow.cores * 0.60
    output:
        targz=temp("{dbs}/taxdump.tar.gz"),
        nodes=temp("{dbs}/nodes.dmp"),
        names=temp("{dbs}/names.dmp")
    shell:
        "curl https://ftp.ncbi.nih.gov/pub/taxonomy/taxdump.tar.gz -o {output.targz}\n"
        "cd {dbs}/ && tar -xzf taxdump.tar.gz nodes.dmp names.dmp"

rule download_ncbi_acc2tax:
    # a bit dirty to prevent mulitple downloads to run at same time
    threads: workflow.cores * 0.60
    output: temp("{dbs}/prot.accession2taxid.FULL.gz")
    shell: "curl https://ftp.ncbi.nlm.nih.gov/pub/taxonomy/accession2taxid/prot.accession2taxid.FULL.gz -o {output}"

      
rule download_uniref:
    # a bit dirty to prevent mulitple downloads to run at same time
    threads: workflow.cores * 0.60
    output: temp("{dbs}/uniref50.fasta.gz")
    shell:
        "curl https://ftp.uniprot.org/pub/databases/uniprot/uniref/uniref50/uniref50.fasta.gz -o {output}"
         

rule download_rvdb:
    # a bit dirty to prevent mulitple downloads to run at same time
    threads: workflow.cores * 0.60
    output: temp("{dbs}/rvdb100.faa.xz"),  # do not mark temp(), could be userspecified location
    shell:
        """
        set +o pipefail;
        url=https://rvdb-prot.pasteur.fr/
        db=$(curl -fs $url | grep -oPm1 'files/U-RVDBv[0-9.]+-prot.fasta.xz')
        curl $url/$db -o {output}
        """

## DEPRECATED - work with NR
# rule download_nr:
#     # a bit dirty to prevent mulitple downloads to run at same time
#     threads: workflow.cores * 0.60
#     output: temp("{dbs}/nr.faa.gz")
#     shell: "curl https://ftp.ncbi.nlm.nih.gov/blast/db/FASTA/nr.gz -o nr.faa.gz"

## databases prep ------------------------------------------------------------##
rule setup_uniref:
    input:
        faagz="{dbs}/uniref50.fasta.gz", # do not mark temp(), could be userspecified location
        nodes="{dbs}/nodes.dmp",
        names="{dbs}/names.dmp"
    output:
        taxonmap=temp("{dbs}/uniref50-acc2tax.tsv.gz"),
        dmnd="{dbs}/uniref50.dmnd"
    shell:
        """
        ( echo -e "accession.version\ttaxid";
          gzip -cd {input.faagz} |
          perl -ne 'if(/>UniRef50_(\S+).*TaxID=(\d+)/){{print $1,"\t",$2,"\n"}}'
        ) | gzip -c > {output.taxonmap}
        """
        'diamond makedb --in {input.faagz} --db {output.dmnd}'
        ' --taxonmap {output.taxonmap} --taxonnodes {input.nodes} --taxonnames {input.names}'

rule unpack_rvdb:
    # a bit dirty to prevent mulitple downloads to run at same time
    input:
        xz="{dbs}/rvdb100.faa.xz",
    output:
        faa=temp("{dbs}/rvdb100.faa")
    shell:
        """
        # unpack
        xz -cd {input.xz} | seqkit seq -m 60 - | seqkit rmdup -s - | perl -pe 's/^>(acc\|\w+\|)([^|]+)/>$2 $1$2/' > {output.faa}
        """
        
# 16GB RAM should suffice: "Approximated maximum memory consumption: 10179M"
rule cluster_rvdb:
    input:
        "{dbs}/rvdb100.faa"
    output:
        faa=temp("{dbs}/rvdb80.faa"),
        cls=temp("{dbs}/rvdb80.faa.clstr"),
	faagz=temp("{dbs}/rvdb80.faa.gz")
    threads: workflow.cores
    shell:
        """
        cd-hit -c .8 -i {input} -o {output.faa} -M 16000 -T {threads}
	gzip -k {output.faa}
        """

rule prep_rvdb_taxonomy:
    # a bit dirty to prevent mulitple downloads to run at same time
    threads: workflow.cores * 0.60
    input:
        faa="{dbs}/rvdb80.faa",
        mapgz="{dbs}/prot.accession2taxid.FULL.gz"
    output:
        acc=temp("{dbs}/rvdb80-acc.tsv"),
        tax=temp("{dbs}/rvdb80-acc2tax.tsv"),
        taxgz=temp("{dbs}/rvdb80-acc2tax.tsv.gz")
    threads: 10
    shell:
        """
        grep '^>' {input.faa} | cut -f3 -d'|' > {output.acc} 
        csvtk -tT grep -j {threads} -P {output.acc} {input.mapgz} > {output.tax}
        gzip -k {output.tax}
        """

rule setup_rvdb:
    input:
        faagz="{dbs}/rvdb80.faa.gz",
        mapgz="{dbs}/rvdb80-acc2tax.tsv.gz",
        nodes="{dbs}/nodes.dmp",
        names="{dbs}/names.dmp"
    output:
        dmnd="{dbs}/rvdb80.dmnd"
    shell:
        'diamond makedb --in {input.faagz} --db {output.dmnd} --taxonmap {input.mapgz} --taxonnodes {input.nodes} --taxonnames {input.names}'

rule setup_retrodb:
    input:
        rvdb_faagz="{dbs}/rvdb80.faa.gz",
        rvdb_taxgz="{dbs}/rvdb80-acc2tax.tsv.gz",
        uniref_faagz="{dbs}/uniref50.fasta.gz",
        uniref_taxgz="{dbs}/uniref50-acc2tax.tsv.gz",
        nodes="{dbs}/nodes.dmp",
        names="{dbs}/names.dmp"
    output:
        taxgz=temp("{dbs}/rvdb80-uniref50-acc2tax.tsv.gz"),
        dmnd="{dbs}/rvdb80-uniref50.dmnd"
    shell:
        """
        cp {input.rvdb_taxgz} {output.taxgz}
        zcat {input.uniref_taxgz} | sed -n '2,$p' | gzip -c >> {output.taxgz}
        zcat {input.rvdb_faagz} {input.uniref_faagz} |
        diamond makedb --db {output.dmnd} --taxonmap {output.taxgz} --taxonnodes {input.nodes} --taxonnames {input.names}
        """

## DEPRECATED
# rule setup_nr:
#     input:
#         faagz="{dbs}/nr.faa.gz",
#         mapgz="{dbs}/prot.accession2taxid.FULL.gz",
#         nodes="{dbs}/nodes.dmp",
#         names="{dbs}/names.dmp",
#     output:
#         dmnd="{dbs}/nr.dmnd"
#     shell:
#         'diamond makedb --in {input.faagz} --db {output.dmnd} --taxonmap {input.mapgz} --taxonnodes {input.nodes} --taxonnames {input.names}'
