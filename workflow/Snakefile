# relative to workflow dir
configfile: "config/config.yaml"

import pandas as pd
import glob
import requests
import re
import os
from os.path import exists

# read accessions for genome download
def acc():
    df = pd.read_csv("genomes.tsv", sep="\t")
    tsv_acc = df['#accession'].tolist()

    fna_acc = glob.glob("genomes/*.fna")
    fna_acc = [x.removeprefix("genomes/").removesuffix(".fna") for x in fna_acc]

    return list(set(tsv_acc + fna_acc))

def check_dbs():
    print("\n# Checking required reference databases ----#")
    
    for db in [config["search"]["db"], config["retrosearch"]["db"]]:
        if exists(db) :
            print("- " + db + " .. ok")
        else:
            print("\nERROR: Reference database `" + db + "` missing" +
                  "\nRun `snakemake setup` to download and prepare the reference databases\n\n")
            os._exit(1)

    shell("touch resources/.check_db")
    return("resources/.check_db")

## Pseudo-rules
rule all:
    input:
        # check_dbs(),
        expand("results/{asm}-validatEVEs.{ext}", asm=acc(), ext=["fna", "pdf", "tsv"])

rule setup:
    input:
        "resources/uniref50.dmnd",
        "resources/rvdb80.dmnd",
        "resources/rvdb80-uniref50.dmnd"
    
## genome download ----------------------------------------------------------##
rule download_genome:
    output: "genomes/{asm}.fna"
    # a bit dirty to prevent mulitple downloads to run at same time
    threads: workflow.cores * 0.60
    shell:
        "(cd genomes && ../workflow/scripts/download-ncbi-genomes.pl {wildcards.asm})"

## analysis -----------------------------------------------------------------##
rule search_assemblies:
    input: "genomes/{asm}.fna"
    output: "results/{asm}-search.o6"
    threads: workflow.cores
    shell:
        "workflow/scripts/diamond-chopped.sh blastx "
        "--query {input} --out {output} --threads {threads} "
        "--db {config[search][db]} --evalue {config[search][evalue]} "
        "-W {config[search][chop_window]} -S {config[search][chop_step]} "
        "{config[search][other_args]}\n"

rule clean_search_hits:
    input: "results/{asm}-search.o6"
    output: "results/{asm}-search-cleaned.o6"
    shell:
        "uniq {input} | perl -ane 'print if $F[3] > {config[search][min_length_aa]}' > {output}"
        
rule extract_putatEVEs:
    input:
        fna="genomes/{asm}.fna",
        o6="results/{asm}-search-cleaned.o6"
    output: "results/{asm}-putatEVEs.fna"
    shell:
        "workflow/scripts/blast2bed -q {input.o6} | seqkit subseq --bed /dev/fd/0 {input.fna} > {output}"

rule retrosearch_putatEVEs:
    input: "results/{asm}-putatEVEs.fna" 
    output: "results/{asm}-retro.o6"
    threads: workflow.cores
    shell:
        "diamond blastx --query {input} --out {output} --threads {threads} "
        "--db {config[retrosearch][db]} --evalue {config[retrosearch][evalue]} "
        "{config[retrosearch][other_args]} "

rule validate_putatEVEs:
    input: "results/{asm}-retro.o6"
    output:
        tsv="results/{asm}-validatEVEs.tsv",
        pdf="results/{asm}-validatEVEs.pdf"
    shell:
        "workflow/scripts/validate.R {input} {output.tsv} {output.pdf}"

rule extract_validatEVEs:
    input:
        fna="results/{asm}-putatEVEs.fna",
        tsv="results/{asm}-validatEVEs.tsv"
    output: "results/{asm}-validatEVEs.fna" 
    shell:
        "cut -f1 {input.tsv} | seqkit grep -f - {input.fna} > {output}"


## databases -----------------------------------------------------------------##
rule download_ncbi_taxonomy:
    # a bit dirty to prevent mulitple downloads to run at same time
    threads: workflow.cores * 0.60
    output:
        targz=temp("resources/taxdump.tar.gz"),
        nodes="resources/nodes.dmp",
        names="resources/names.dmp"
    shell:
        "curl https://ftp.ncbi.nih.gov/pub/taxonomy/taxdump.tar.gz -o {output.targz}\n"
        "cd resources/ && tar -xzf taxdump.tar.gz nodes.dmp names.dmp"

rule download_ncbi_acc2tax:
    # a bit dirty to prevent mulitple downloads to run at same time
    threads: workflow.cores * 0.60
    output: "resources/ncbi-acc2tax.gz"
    shell: "curl https://ftp.ncbi.nlm.nih.gov/pub/taxonomy/accession2taxid/prot.accession2taxid.FULL.gz -o {output}"

rule download_nr:
    # a bit dirty to prevent mulitple downloads to run at same time
    threads: workflow.cores * 0.60
    output: "resources/nr.faa.gz"
    shell: "curl https://ftp.ncbi.nlm.nih.gov/blast/db/FASTA/nr.gz -o nr.faa.gz"

rule setup_nr:
    input:
        faagz="resources/nr.faa.gz",
        mapgz="resources/ncbi-acc2tax.gz",
        nodes="resources/nodes.dmp",
        names="resources/names.dmp",
    output:
        dmnd="resources/nr.dmnd"
    shell:
        'diamond makedb --in {input.faagz} --db {output.dmnd} --taxonmap {input.mapgz} --taxonnodes {input.nodes} --taxonnames {input.names}'
           
rule download_uniref:
    # a bit dirty to prevent mulitple downloads to run at same time
    threads: workflow.cores * 0.60
    output: "resources/uniref50.fasta.gz"
    shell:
        "curl https://ftp.uniprot.org/pub/databases/uniprot/uniref/uniref50/uniref50.fasta.gz -o {output}"
         
rule setup_uniref:
    input:
        faagz="resources/uniref50.fasta.gz",
        nodes="resources/nodes.dmp",
        names="resources/names.dmp"
    output:
        taxonmap=temp("resources/uniref50-acc2tax.tsv.gz"),
        dmnd="resources/uniref50.dmnd"
    shell:
        """
        ( echo -e "accession.version\ttaxid";
          gzip -cd {input.faagz} |
          perl -ne 'if(/>UniRef50_(\S+).*TaxID=(\d+)/){{print $1,"\t",$2,"\n"}}'
        ) | gzip -c > {output.taxonmap}
        """
        'diamond makedb --in {input.faagz} --db {output.dmnd}'
        ' --taxonmap {output.taxonmap} --taxonnodes {input.nodes} --taxonnames {input.names}'

rule download_rvdb:
    # a bit dirty to prevent mulitple downloads to run at same time
    threads: workflow.cores * 0.60
    output:
        xz=temp("resources/rvdb100.faa.xz"),
        faa=temp("resources/rvdb100.faa")
    shell:
        """
        set +o pipefail;
        url=https://rvdb-prot.pasteur.fr/
        db=$(curl -fs $url | grep -oPm1 'files/U-RVDBv[0-9.]+-prot.fasta.xz')
        curl $url/$db -o {output.xz}
        # unpack
        xz -cd {output.xz} | seqkit seq -m 60 - | seqkit rmdup -s - > {output.faa}
        """

# 12GB RAM should suffice: "Approximated maximum memory consumption: 10179M"
rule cluster_rvdb:
    input:
        "resources/rvdb100.faa"
    output:
        faa=temp("resources/rvdb80r.faa"),
        cls=temp("resources/rvdb80r.faa.clstr")
    threads: workflow.cores
    shell:
        """
        cd-hit -c .8 -i {input} -o {output.faa} -M 12000 -T {threads}
        """

# DEPRECATED - can get taxinfo from taxified diamond DB
# rule download_rvdb_taxonomy:
#     # a bit dirty to prevent mulitple downloads to run at same time
#     threads: workflow.cores * 0.60
#     input:
#         "resources/rvdb80r.faa"
#     output:
#         acc=temp("resources/rvdb80-acc.tsv"),
#         tax=temp("resources/rvdb80-acc2tax.tsv")
#     threads: 10
#     shell:
#         """
#         grep '^>' {input} | cut -f3 -d'|' > {output.acc} 
#         curl https://ftp.ncbi.nlm.nih.gov/pub/taxonomy/accession2taxid/prot.accession2taxid.FULL.gz |
#         csvtk -tT grep -j {threads} -P {output.acc} > {output.tax}
#         """

# DEPRECATED - can get taxinfo from taxified diamond DB
# rule taxify_rvdb:
#     input:
#         faa="resources/rvdb80r.faa",
#         tax="resources/rvdb80-acc2tax.tsv"
#     output:
#         faa=temp("resources/rvdb80.faa"),
#         gz="resources/rvdb80.faa.gz"
#     threads: 10
#     shell:
#         """
#         seqkit replace -j {threads} -p 'acc\\|[^\\|]+\\|([^\\|]+).*' -r '${{1}} ${{0}} taxid={{kv}}' -k {input.tax} {input.faa} -m NA > {output.faa}
#         gzip -k {output.faa}
#         """

rule setup_rvdb:
    input:
        faagz="resources/rvdb80.faa.gz",
        mapgz="resources/ncbi-acc2tax.gz"
        nodes="resources/nodes.dmp",
        names="resources/names.dmp"
    output:
        dmnd="resources/rvdb80.dmnd"
    shell:
        'diamond makedb --in {input.faagz} --db {output.dmnd} --taxonmap {input.mapgz} --taxonnodes {input.nodes} --taxonnames {input.names}'

rule setup_retrodb:
    input:
        rvdb_faagz="resources/rvdb80.faa.gz",
        rvdb_taxgz="resources/rvdb80-acc2tax.tsv.gz",
        uniref_faagz="resources/uniref50.fasta.gz",
        uniref_taxgz="resources/uniref50-acc2tax.tsv.gz",
        nodes="resources/nodes.dmp",
        names="resources/names.dmp"
    output:
        taxgz=temp("resources/rvdb80-uniref50-acc2tax.tsv.gz"),
        dmnd="resources/rvdb80-uniref50.dmnd"
    shell:
        """
        cp {input.rvdb_taxgz} {output.taxgz}
        zcat {input.uniref_taxgz} | sed -n '2,$p' | gzip -c >> {output.taxgz}
        zcat {input.rvdb_faagz} {input.uniref_faagz} |
        diamond makedb --db {output.dmnd} --taxonmap {output.taxgz} --taxonnodes {input.nodes} --taxonnames {input.names}
        """
